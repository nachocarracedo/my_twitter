{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carrai1\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from twython import Twython\n",
    "import settings #twitter keys: APP_KEY, APP_SECRET, OAUTH_TOKEN, OAUTH_TOKEN_SECRET\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import string\n",
    "from collections import defaultdict\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import sklearn\n",
    "\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-01 \n",
      "\n",
      "CPython 3.6.0\n",
      "IPython 5.1.0\n",
      "\n",
      "pandas 0.19.2\n",
      "scipy 0.18.1\n",
      "matplotlib 2.0.0\n",
      "twython 3.6.0\n",
      "nltk 3.2.2\n",
      "wordcloud 1.3.1\n",
      "gensim 2.3.0\n",
      "\n",
      "compiler   : MSC v.1900 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 10\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel\n",
      "CPU cores  : 8\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -d -v -m -p pandas,scipy,matplotlib,twython,nltk,wordcloud,gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of following:  616\n"
     ]
    }
   ],
   "source": [
    "# twython auth\n",
    "twitter = Twython(settings.APP_KEY, settings.APP_SECRET,settings.OAUTH_TOKEN, settings.OAUTH_TOKEN_SECRET)\n",
    "# init empty lists to save tweets and metadata\n",
    "user_ids, user_names, texts , creation, retweets ,favorites,lenguage, retweet, retweet_from, in_reply, coordinates = ([] for i in range(11))\n",
    "# get following IDs and NAMES (can get more info of users if needed!)\n",
    "following_ids = [] # to save ids\n",
    "following_names = {} # dictinary key:user_id, value: user_name\n",
    "user_location = []\n",
    "following = twitter.get_friends_ids()[\"ids\"]\n",
    "print(\"Number of following: \",len(following))\n",
    "\n",
    "#save details of the account\n",
    "#Add details as username and number of people following.\n",
    "name = twitter.verify_credentials()['name']\n",
    "date_creation = twitter.verify_credentials()['created_at']\n",
    "nfollowing = twitter.verify_credentials()['friends_count']\n",
    "nfollowers = twitter.verify_credentials()['followers_count']\n",
    "\n",
    "# get 200 tweets and metadata from each friend (can get more metadata if needed!)\n",
    "for user_id in following:\n",
    "    tweets200 = twitter.get_user_timeline(user_id=user_id,count=50)\n",
    "    for t in tweets200:\n",
    "        user_ids.append(user_id)\n",
    "        #user_names.append(following_names[user_id])\n",
    "        texts.append(t[\"text\"])\n",
    "        creation.append(t[\"created_at\"])\n",
    "        retweets.append(t[\"retweet_count\"])\n",
    "        favorites.append(t[\"favorite_count\"])\n",
    "        lenguage.append(t[\"lang\"])\n",
    "        #coordinates.append(t[\"coordinates\"])\n",
    "        in_reply.append(t['in_reply_to_screen_name'])\n",
    "        retweet.append(('retweeted_status') in t)\n",
    "        if ('retweeted_status') in t:\n",
    "            retweet_from.append(t['retweeted_status']['user']['name'])\n",
    "        else:\n",
    "            retweet_from.append(\"N/A\")\n",
    "    #get user location\n",
    "    user_location.append(twitter.show_user(user_id=user_id)[\"location\"])\n",
    "    \n",
    "# create final DataFrame\n",
    "mytweets = pd.DataFrame({'user_id':user_ids,#'user_name':user_names,\n",
    "                         'text':texts ,'retweet': retweet, 'creation':creation,\n",
    "                         'retweets':retweets , 'favorites':favorites, \n",
    "                         'lenguage':lenguage,'retweet_from': retweet_from,\n",
    "                         'in_reply':in_reply})\n",
    "\n",
    "mytweets[\"in_reply\"] = mytweets[\"in_reply\"].map(lambda x: \"None\" if x is None else x) # fix in_replay column None type to \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>creation</th>\n",
       "      <th>favorites</th>\n",
       "      <th>in_reply</th>\n",
       "      <th>lenguage</th>\n",
       "      <th>retweet</th>\n",
       "      <th>retweet_from</th>\n",
       "      <th>retweets</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fri Sep 01 14:30:23 +0000 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1</td>\n",
       "      <td>Fenway needs more R&amp;amp;D space, developer say...</td>\n",
       "      <td>12705772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fri Sep 01 13:49:01 +0000 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>DA says officer justified in fatally shooting ...</td>\n",
       "      <td>12705772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Sep 01 13:48:38 +0000 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "      <td>Hubway</td>\n",
       "      <td>5</td>\n",
       "      <td>RT @Hubway: 193,000+ trips taken on #Hubway in...</td>\n",
       "      <td>12705772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fri Sep 01 13:12:17 +0000 2017</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>Students in one Harvard course being told they...</td>\n",
       "      <td>12705772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fri Sep 01 13:10:49 +0000 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>Man hit and killed by outbound train in Beverl...</td>\n",
       "      <td>12705772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         creation  favorites in_reply lenguage retweet  \\\n",
       "0  Fri Sep 01 14:30:23 +0000 2017          1     None       en   False   \n",
       "1  Fri Sep 01 13:49:01 +0000 2017          0     None       en   False   \n",
       "2  Fri Sep 01 13:48:38 +0000 2017          0     None       en    True   \n",
       "3  Fri Sep 01 13:12:17 +0000 2017          3     None       en   False   \n",
       "4  Fri Sep 01 13:10:49 +0000 2017          0     None       en   False   \n",
       "\n",
       "  retweet_from  retweets                                               text  \\\n",
       "0          N/A         1  Fenway needs more R&amp;D space, developer say...   \n",
       "1          N/A         0  DA says officer justified in fatally shooting ...   \n",
       "2       Hubway         5  RT @Hubway: 193,000+ trips taken on #Hubway in...   \n",
       "3          N/A         0  Students in one Harvard course being told they...   \n",
       "4          N/A         0  Man hit and killed by outbound train in Beverl...   \n",
       "\n",
       "    user_id  \n",
       "0  12705772  \n",
       "1  12705772  \n",
       "2  12705772  \n",
       "3  12705772  \n",
       "4  12705772  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2438, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mytweets.to_csv(\"./data/mytweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pd.read_csv(\"./data/mytweets.csv\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Home many of all tweets are regular, tweeet, reply\n",
    "def tweet_type(row):\n",
    "    if row.retweet == True:\n",
    "        return \"retweet\"\n",
    "    elif row.in_reply == \"None\":\n",
    "        return \"reply\"\n",
    "    else:\n",
    "        return \"tweet\"\n",
    "# are there any replies and retweets?\n",
    "type_tweet = mytweets.apply(tweet_type,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reply      1241\n",
       "retweet     769\n",
       "tweet       428\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_tweet.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english') + stopwords.words('spanish')\n",
    "other_stop = ['via','&amp;', 'now','one','thing','us', 'will',\"it's\",'it',\n",
    "              \"i'm\",\"u\",\"you\",\"yet\", \"say\",\"much\", \"gt\", \"new\", \"us\", \"also\",\"don't\"]\n",
    "stop = set(stop + other_stop)\n",
    "punctuationset = set(string.punctuation)\n",
    "\n",
    "def clean_tweets (tweets_string, punctuation=True, lemmatize=False, stopwords=True):\n",
    "    \"\"\" Gets a string with all tweets and remove RT, links, and ids(start with @).\n",
    "    OPTIONS: \n",
    "    punctuation: if true removes punctuation (true by default)\n",
    "    lemmatize: if true gets the lemmas of words (false by default)\n",
    "    stopwords: if true we remove stopwords (true by default). \n",
    "    \"\"\"\n",
    "    \n",
    "    # If it's a a link, a twitter user or RT, we remove it.\n",
    "    words_clean = \" \".join([word.lower() for word in tweets_string.split()\n",
    "                            if 'http' not in word\n",
    "                            and not word.startswith('@')\n",
    "                            and word != 'RT'\n",
    "                            ])   \n",
    "    \n",
    "    # options\n",
    "    if stopwords:\n",
    "        words_clean = \" \".join([w for w in words_clean.split() if w not in stop])\n",
    "        \n",
    "    #print(words_clean.lower().split())\n",
    "    if punctuation:\n",
    "        words_clean = \"\".join([w for w in words_clean if w not in punctuationset])\n",
    "        \n",
    "    if lemmatize : \n",
    "            lemma = WordNetLemmatizer() \n",
    "            words_clean = \" \".join(lemma.lemmatize(word) \n",
    "                                   for word in words_clean.split()\n",
    "                                   if type(word) is str)\n",
    "            \n",
    "    return words_clean\n",
    "\n",
    "    \n",
    "# words of ALL TWEETS\n",
    "words = clean_tweets(' '.join(mytweets['text']), lemmatize=False)\n",
    "# words of ALL TWEETS keeping # for hashtags\n",
    "words_hash = clean_tweets(' '.join(mytweets['text']), lemmatize=False, punctuation=False)\n",
    "# words of ALL REGULAR TWEETS \n",
    "words_regular = clean_tweets(' '.join(mytweets[(mytweets[\"in_reply\"]==\"None\") \n",
    "                                               & (mytweets[\"retweet\"]==False)].text),\n",
    "                             lemmatize=False)\n",
    "# words of ONLY RETWEETS\n",
    "words_rt = clean_tweets(' '.join(mytweets[mytweets[\"retweet\"] == True].text),\n",
    "                        lemmatize=False)\n",
    "# words of ONLY REPLIES\n",
    "words_reply = clean_tweets(' '.join(mytweets[mytweets[\"in_reply\"] != \"None\"].text),\n",
    "                           lemmatize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en      21211\n",
       "es       2894\n",
       "fail      694\n",
       "fr        665\n",
       "it        655\n",
       "af        517\n",
       "ca        447\n",
       "nl        421\n",
       "da        322\n",
       "pt        278\n",
       "no        227\n",
       "so        198\n",
       "ro        187\n",
       "cy        182\n",
       "et        171\n",
       "sv        154\n",
       "tl        149\n",
       "id        144\n",
       "fi         97\n",
       "sl         82\n",
       "pl         67\n",
       "tr         59\n",
       "de         58\n",
       "lt         52\n",
       "sw         42\n",
       "hr         41\n",
       "sk         40\n",
       "sq         36\n",
       "hu         34\n",
       "cs         29\n",
       "lv         28\n",
       "he         24\n",
       "vi         18\n",
       "ja         11\n",
       "ru          5\n",
       "bg          1\n",
       "el          1\n",
       "ar          1\n",
       "Name: lenguage, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def leng(row):\n",
    "    text = clean_tweets(row['text'])\n",
    "    #text = row['text_clean']\n",
    "    #print(text)\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except Exception as e:\n",
    "        return 'fail'\n",
    "\n",
    "mytweets[\"lenguage\"] = mytweets.apply(leng,1)\n",
    "mytweets[\"lenguage\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mytweets.to_csv(\"./data/mytweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##mytweets = pd.read_csv(\"./data/mytweets.csv\", encoding = \"ISO-8859-1\")\n",
    "#mytweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top(xs, top=-1):\n",
    "    \"\"\"Gets words that show up more, option 'top' limits to that number\"\"\"\n",
    "    counts = defaultdict(int)\n",
    "    for x in xs:\n",
    "        counts[x] += 1\n",
    "    return sorted(counts.items(), reverse=True, key=lambda tup: tup[1])[:top]\n",
    "\n",
    "# hashtags\n",
    "#words = clean_tweets(' '.join(mytweets['text']), puntuation=False,sentiment=False) # puntuation is False so we don't delete hastags\n",
    "top20hash = pd.DataFrame(top([x for x in words_hash.split() if x[0]=='#' and x!='#rt'], top=20),\n",
    "                         columns=[\"hashtag\", \"count\"])\n",
    "# words\n",
    "top20words = pd.DataFrame(top([x for x in words.split()], top=20),\n",
    "                          columns=[\"word\", \"count\"])\n",
    "\n",
    "\n",
    "# location of followings (count)\n",
    "locations = top(user_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get more info about the location using google maps API\n",
    "list_add=[] # list of lists [localization, number_of_times, latitude, longitude]\n",
    "\n",
    "for loc in locations:\n",
    "    la=[loc[0], loc[1]]\n",
    "    fixed_address = loc[0].replace(\" \",\"+\")\n",
    "    url = 'https://maps.googleapis.com/maps/api/geocode/json?address=' + fixed_address\n",
    "    try:        \n",
    "        response = requests.get(url)\n",
    "        resp_json_payload = response.json()\n",
    "        coordinates = resp_json_payload['results'][0]['geometry']['location']\n",
    "        la.append(str(coordinates['lat'])+\" \"+str(coordinates['lng']))\n",
    "        list_add.append(la)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "location_df = pd.DataFrame(list_add, columns=[\"location\",\"count\", \"coordinates\"])\n",
    "location_plot = location_df.groupby(\"coordinates\")[\"count\"].sum()\n",
    "location_plot.sort_values(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression (many models were tested using CV, lg was the best performing one)\n",
    "sa_train = pd.read_csv(\".\\\\data\\\\Sentiment Analysis Dataset.csv\",error_bad_lines=False)\n",
    "x_sa = sa_train[\"SentimentText\"]\n",
    "y_sa = sa_train[\"Sentiment\"]\n",
    "del sa_train\n",
    "x_sa_clean = x_sa.map(clean_tweets)\n",
    "# vectorize (bag of words)\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "X = vectorizer.fit_transform(x_sa)\n",
    "# fit the whole training set and pred for my tweets to check\n",
    "#lge = LogisticRegression(random_state=23, fit_intercept=True, C=0.5, class_weight='balanced')\n",
    "# get Train and my data ready for the molel\n",
    "mytweets_eng = mytweets[mytweets['lenguage'] == 'en']\n",
    "x_unseen = vectorizer.transform(mytweets_eng['text'].map(clean_tweets))\n",
    "#lge.fit(X, y_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save model \n",
    "#filename = 'lg_sa_model.sav'\n",
    "#pickle.dump(lge, open('./models/'+filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Prob</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.204894702665, 0.795105297335]</td>\n",
       "      <td>@mark_riedl @risi1979 Well this paper is good,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.294197324483, 0.705802675517]</td>\n",
       "      <td>Here’s every total solar eclipse happening in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.319471135369, 0.680528864631]</td>\n",
       "      <td>RT @FrankUnderwocd: Life imitates art.\\n#G20Su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.262969061058, 0.737030938942]</td>\n",
       "      <td>RT @Cyan4973: Zstandard v1.3.0 released : http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0713107007541, 0.928689299246]</td>\n",
       "      <td>Modern transitor fab process insightful commen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.861468285086, 0.138531714914]</td>\n",
       "      <td>@IgorCarron Nuclear energy kills animals all t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.374464699459, 0.625535300541]</td>\n",
       "      <td>RT @soumithchintala: :D between this paper and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0903359806808, 0.909664019319]</td>\n",
       "      <td>RT @DevilleSy: The school fair, where you buy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.15186841336, 0.84813158664]</td>\n",
       "      <td>Aquila's successful second flight, with a vide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.863628512045, 0.136371487955]</td>\n",
       "      <td>RT @mblondel_ml: Preprint of our paper on soft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.242924751495, 0.757075248505]</td>\n",
       "      <td>Measuring the Progress of AI Research https://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.116831147768, 0.883168852232]</td>\n",
       "      <td>@karpathy Wow, congrats Andrej!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.827383362749, 0.172616637251]</td>\n",
       "      <td>@rodolfor though it's not the intrinsic diffic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.227886315696, 0.772113684304]</td>\n",
       "      <td>@rodolfor Easier then, beauty is in the eye of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.466857978231, 0.533142021769]</td>\n",
       "      <td>@rodolfor Totally disagree on this. :) You can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.181617467637, 0.818382532363]</td>\n",
       "      <td>RT @d1ca1: Writeup analyzing the #NIPS2016 rev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.387558338627, 0.612441661373]</td>\n",
       "      <td>RT @DannyDutch: #Renaissance paintings recreat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.127295414617, 0.872704585383]</td>\n",
       "      <td>RT @preskill: One of my favorite lines: \"I hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.196326156649, 0.803673843351]</td>\n",
       "      <td>RT @aishfenton: Love this viz of mcmc methods....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.38320461641, 0.61679538359]</td>\n",
       "      <td>RT @garibaldu: Machine Learning glows as an ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.406167786664, 0.593832213336]</td>\n",
       "      <td>RT @LukeBornn: Best statistics article I've re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.097669037651, 0.902330962349]</td>\n",
       "      <td>RT @jurafsky: Speech and Language Processing 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.363986729211, 0.636013270789]</td>\n",
       "      <td>RT @haldaume3: @adam_will_do_it @mark_riedl i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.415583843817, 0.584416156183]</td>\n",
       "      <td>RT @haldaume3: @mark_riedl yup :) I oft tell S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.152704541784, 0.847295458216]</td>\n",
       "      <td>RT @dwf: Welp, didn't expect that to blow up l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.484945678318, 0.515054321682]</td>\n",
       "      <td>RT @PaulFox13: This is insane. #houstonflood h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.275866691749, 0.724133308251]</td>\n",
       "      <td>RT @xtimv: \"The useless beauty of Reinforce\" b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.496387466451, 0.503612533549]</td>\n",
       "      <td>RT @thehill: Texas senators who voted against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.544031578837, 0.455968421163]</td>\n",
       "      <td>RT @WithTheBest: .@hugo_larochelle will presen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.612631366569, 0.387368633431]</td>\n",
       "      <td>RT @Caltech: Can't tell a striker from a right...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.737019151484, 0.262980848516]</td>\n",
       "      <td>RT @denial101x: \"This just happened\" Associate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.688321418876, 0.311678581124]</td>\n",
       "      <td>RT @doctorwhy: This is what censorship looks l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.329051268092, 0.670948731908]</td>\n",
       "      <td>RT @AcademicsSay: summer writing list | @dorri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.517300487587, 0.482699512413]</td>\n",
       "      <td>RT @SenKamalaHarris: Let’s get one thing strai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.768259232386, 0.231740767614]</td>\n",
       "      <td>RT @WiMLworkshop: Just 16 days left until abst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.136335987037, 0.863664012963]</td>\n",
       "      <td>RT @hmason: If you're interested in machine le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.392885541586, 0.607114458414]</td>\n",
       "      <td>RT @peterseibel: Maybe Gladwell’s problem is h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.146275921137, 0.853724078863]</td>\n",
       "      <td>RT @mat_johnson: Looking at live tweets of a T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0973891459435, 0.902610854057]</td>\n",
       "      <td>RT @physicsteo: @rguha .@A_Aspuru_Guzik recomm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.517074586539, 0.482925413461]</td>\n",
       "      <td>RT @beenwrekt: The schedule and live stream ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.545417570849, 0.454582429151]</td>\n",
       "      <td>RT @halhod: Working on a piece about supercond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.303471133641, 0.696528866359]</td>\n",
       "      <td>RT @googleresearch: Get more insight into the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.418596107828, 0.581403892172]</td>\n",
       "      <td>RT @SeanMcElwee: If you're frantically trying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.411341925655, 0.588658074345]</td>\n",
       "      <td>RT @hugo_larochelle: My slides for my talk at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.177948037569, 0.822051962431]</td>\n",
       "      <td>RT @deray: You can watch the #SolarEclipse2017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0715448324817, 0.928455167518]</td>\n",
       "      <td>RT @shakir_za: @thejaan P.s. I'm a huge fan of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.328782441049, 0.671217558951]</td>\n",
       "      <td>RT @thejaan: How does physics connect to machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.721305257709, 0.278694742291]</td>\n",
       "      <td>RT @MrGeorgeWallace: BREAKING: New England pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.483802898782, 0.516197101218]</td>\n",
       "      <td>RT @Mikel_Jollett: Removing monuments to slave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.707822723247, 0.292177276753]</td>\n",
       "      <td>RT @DearWhitePeople: Slavery. Slavery is worse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Prediction                               Prob  \\\n",
       "1007           1   [0.204894702665, 0.795105297335]   \n",
       "1009           1   [0.294197324483, 0.705802675517]   \n",
       "1010           1   [0.319471135369, 0.680528864631]   \n",
       "1011           1   [0.262969061058, 0.737030938942]   \n",
       "1012           1  [0.0713107007541, 0.928689299246]   \n",
       "1015           0   [0.861468285086, 0.138531714914]   \n",
       "1017           1   [0.374464699459, 0.625535300541]   \n",
       "1018           1  [0.0903359806808, 0.909664019319]   \n",
       "1019           1     [0.15186841336, 0.84813158664]   \n",
       "1020           0   [0.863628512045, 0.136371487955]   \n",
       "1022           1   [0.242924751495, 0.757075248505]   \n",
       "1023           1   [0.116831147768, 0.883168852232]   \n",
       "1024           0   [0.827383362749, 0.172616637251]   \n",
       "1025           1   [0.227886315696, 0.772113684304]   \n",
       "1026           1   [0.466857978231, 0.533142021769]   \n",
       "1028           1   [0.181617467637, 0.818382532363]   \n",
       "1029           1   [0.387558338627, 0.612441661373]   \n",
       "1030           1   [0.127295414617, 0.872704585383]   \n",
       "1032           1   [0.196326156649, 0.803673843351]   \n",
       "1033           1     [0.38320461641, 0.61679538359]   \n",
       "1034           1   [0.406167786664, 0.593832213336]   \n",
       "1035           1   [0.097669037651, 0.902330962349]   \n",
       "1036           1   [0.363986729211, 0.636013270789]   \n",
       "1037           1   [0.415583843817, 0.584416156183]   \n",
       "1038           1   [0.152704541784, 0.847295458216]   \n",
       "1039           1   [0.484945678318, 0.515054321682]   \n",
       "1040           1   [0.275866691749, 0.724133308251]   \n",
       "1041           1   [0.496387466451, 0.503612533549]   \n",
       "1042           0   [0.544031578837, 0.455968421163]   \n",
       "1043           0   [0.612631366569, 0.387368633431]   \n",
       "1044           0   [0.737019151484, 0.262980848516]   \n",
       "1045           0   [0.688321418876, 0.311678581124]   \n",
       "1046           1   [0.329051268092, 0.670948731908]   \n",
       "1047           0   [0.517300487587, 0.482699512413]   \n",
       "1048           0   [0.768259232386, 0.231740767614]   \n",
       "1049           1   [0.136335987037, 0.863664012963]   \n",
       "1050           1   [0.392885541586, 0.607114458414]   \n",
       "1051           1   [0.146275921137, 0.853724078863]   \n",
       "1052           1  [0.0973891459435, 0.902610854057]   \n",
       "1053           0   [0.517074586539, 0.482925413461]   \n",
       "1055           0   [0.545417570849, 0.454582429151]   \n",
       "1056           1   [0.303471133641, 0.696528866359]   \n",
       "1057           1   [0.418596107828, 0.581403892172]   \n",
       "1058           1   [0.411341925655, 0.588658074345]   \n",
       "1059           1   [0.177948037569, 0.822051962431]   \n",
       "1060           1  [0.0715448324817, 0.928455167518]   \n",
       "1061           1   [0.328782441049, 0.671217558951]   \n",
       "1063           0   [0.721305257709, 0.278694742291]   \n",
       "1064           1   [0.483802898782, 0.516197101218]   \n",
       "1065           0   [0.707822723247, 0.292177276753]   \n",
       "\n",
       "                                                  tweet  \n",
       "1007  @mark_riedl @risi1979 Well this paper is good,...  \n",
       "1009  Here’s every total solar eclipse happening in ...  \n",
       "1010  RT @FrankUnderwocd: Life imitates art.\\n#G20Su...  \n",
       "1011  RT @Cyan4973: Zstandard v1.3.0 released : http...  \n",
       "1012  Modern transitor fab process insightful commen...  \n",
       "1015  @IgorCarron Nuclear energy kills animals all t...  \n",
       "1017  RT @soumithchintala: :D between this paper and...  \n",
       "1018  RT @DevilleSy: The school fair, where you buy ...  \n",
       "1019  Aquila's successful second flight, with a vide...  \n",
       "1020  RT @mblondel_ml: Preprint of our paper on soft...  \n",
       "1022  Measuring the Progress of AI Research https://...  \n",
       "1023                    @karpathy Wow, congrats Andrej!  \n",
       "1024  @rodolfor though it's not the intrinsic diffic...  \n",
       "1025  @rodolfor Easier then, beauty is in the eye of...  \n",
       "1026  @rodolfor Totally disagree on this. :) You can...  \n",
       "1028  RT @d1ca1: Writeup analyzing the #NIPS2016 rev...  \n",
       "1029  RT @DannyDutch: #Renaissance paintings recreat...  \n",
       "1030  RT @preskill: One of my favorite lines: \"I hav...  \n",
       "1032  RT @aishfenton: Love this viz of mcmc methods....  \n",
       "1033  RT @garibaldu: Machine Learning glows as an ar...  \n",
       "1034  RT @LukeBornn: Best statistics article I've re...  \n",
       "1035  RT @jurafsky: Speech and Language Processing 3...  \n",
       "1036  RT @haldaume3: @adam_will_do_it @mark_riedl i ...  \n",
       "1037  RT @haldaume3: @mark_riedl yup :) I oft tell S...  \n",
       "1038  RT @dwf: Welp, didn't expect that to blow up l...  \n",
       "1039  RT @PaulFox13: This is insane. #houstonflood h...  \n",
       "1040  RT @xtimv: \"The useless beauty of Reinforce\" b...  \n",
       "1041  RT @thehill: Texas senators who voted against ...  \n",
       "1042  RT @WithTheBest: .@hugo_larochelle will presen...  \n",
       "1043  RT @Caltech: Can't tell a striker from a right...  \n",
       "1044  RT @denial101x: \"This just happened\" Associate...  \n",
       "1045  RT @doctorwhy: This is what censorship looks l...  \n",
       "1046  RT @AcademicsSay: summer writing list | @dorri...  \n",
       "1047  RT @SenKamalaHarris: Let’s get one thing strai...  \n",
       "1048  RT @WiMLworkshop: Just 16 days left until abst...  \n",
       "1049  RT @hmason: If you're interested in machine le...  \n",
       "1050  RT @peterseibel: Maybe Gladwell’s problem is h...  \n",
       "1051  RT @mat_johnson: Looking at live tweets of a T...  \n",
       "1052  RT @physicsteo: @rguha .@A_Aspuru_Guzik recomm...  \n",
       "1053  RT @beenwrekt: The schedule and live stream ar...  \n",
       "1055  RT @halhod: Working on a piece about supercond...  \n",
       "1056  RT @googleresearch: Get more insight into the ...  \n",
       "1057  RT @SeanMcElwee: If you're frantically trying ...  \n",
       "1058  RT @hugo_larochelle: My slides for my talk at ...  \n",
       "1059  RT @deray: You can watch the #SolarEclipse2017...  \n",
       "1060  RT @shakir_za: @thejaan P.s. I'm a huge fan of...  \n",
       "1061  RT @thejaan: How does physics connect to machi...  \n",
       "1063  RT @MrGeorgeWallace: BREAKING: New England pat...  \n",
       "1064  RT @Mikel_Jollett: Removing monuments to slave...  \n",
       "1065  RT @DearWhitePeople: Slavery. Slavery is worse...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model from disk\n",
    "filename = 'lg_sa_model.sav'\n",
    "lgmodel = pickle.load(open('./models/'+filename, 'rb'))\n",
    "predictions_unseen = lgmodel.predict(x_unseen)\n",
    "prob_unseen = lgmodel.predict_proba(x_unseen)\n",
    "results = pd.DataFrame({'Prediction':predictions_unseen,'Prob':list(prob_unseen),'tweet':mytweets_eng['text']})\n",
    "results.iloc[700:750] #0: negative, 1:positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# top 4 languages\n",
    "mytweets_len = mytweets[mytweets[\"lenguage\"]!= 'fail'][\"lenguage\"]\n",
    "len_top4 = mytweets_len.value_counts()[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Number of tweets per date (time series plot)\n",
    "mytweets['date'] = pd.to_datetime(mytweets.creation).dt.date\n",
    "mytweets['year'] = pd.to_numeric(pd.to_datetime(mytweets.creation).dt.year)\n",
    "mytweets['month'] = pd.to_numeric(pd.to_datetime(mytweets.creation).dt.month)\n",
    "\n",
    "import time\n",
    "strings = time.strftime(\"%Y,%m,%d,%H,%M,%S\")\n",
    "t = strings.split(',')\n",
    "numbers = [ int(x) for x in t ]\n",
    "#print(numbers)\n",
    "\n",
    "\n",
    "#filter to last year\n",
    "if numbers[1]==1:\n",
    "    mytweets_last_month = mytweets[(mytweets['year'] == (numbers[0]-1))& (mytweets['month'] == 12)]\n",
    "else:\n",
    "    mytweets_last_month = mytweets[(mytweets['year'] == numbers[0])& (mytweets['month'] == (numbers[1]-1))] \n",
    "    \n",
    "tweets_reply = mytweets_last_month[mytweets_last_month.in_reply != None]\n",
    "tweets_rt = mytweets_last_month[mytweets_last_month.retweet == True]\n",
    "tweets = mytweets_last_month[(mytweets_last_month[\"in_reply\"]==\"None\")& (mytweets_last_month[\"retweet\"]==False)]\n",
    "\n",
    "ts_reply = pd.DataFrame(tweets_reply.groupby(['date']).size()).reset_index()\n",
    "ts_reply.columns=['date','count']\n",
    "ts_rt = pd.DataFrame(tweets_rt.groupby(['date']).size()).reset_index()\n",
    "ts_rt.columns=['date','count']\n",
    "ts = pd.DataFrame(tweets.groupby(['date']).size()).reset_index()\n",
    "ts.columns=['date','count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-03</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-04</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-05</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-08-06</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-08-07</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-08-08</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-08-09</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-08-10</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017-08-11</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017-08-12</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017-08-13</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017-08-19</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017-08-20</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2017-08-21</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2017-08-23</td>\n",
       "      <td>708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2017-08-24</td>\n",
       "      <td>724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2017-08-25</td>\n",
       "      <td>852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2017-08-26</td>\n",
       "      <td>649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2017-08-27</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2017-08-28</td>\n",
       "      <td>1151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2017-08-29</td>\n",
       "      <td>1646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2017-08-30</td>\n",
       "      <td>2187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>3508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  count\n",
       "0   2017-08-01    141\n",
       "1   2017-08-02    157\n",
       "2   2017-08-03    161\n",
       "3   2017-08-04    148\n",
       "4   2017-08-05     95\n",
       "5   2017-08-06    140\n",
       "6   2017-08-07    164\n",
       "7   2017-08-08    233\n",
       "8   2017-08-09    225\n",
       "9   2017-08-10    222\n",
       "10  2017-08-11    224\n",
       "11  2017-08-12    139\n",
       "12  2017-08-13    181\n",
       "13  2017-08-14    194\n",
       "14  2017-08-15    282\n",
       "15  2017-08-16    348\n",
       "16  2017-08-17    379\n",
       "17  2017-08-18    362\n",
       "18  2017-08-19    237\n",
       "19  2017-08-20    215\n",
       "20  2017-08-21    429\n",
       "21  2017-08-22    499\n",
       "22  2017-08-23    708\n",
       "23  2017-08-24    724\n",
       "24  2017-08-25    852\n",
       "25  2017-08-26    649\n",
       "26  2017-08-27    701\n",
       "27  2017-08-28   1151\n",
       "28  2017-08-29   1646\n",
       "29  2017-08-30   2187\n",
       "30  2017-08-31   3508"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apr 26 2011'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_creation[4:10].strip()+date_creation[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,35))\n",
    "\n",
    "#type of tweets\n",
    "ax1 = plt.subplot2grid((8,2), (0,1))           \n",
    "type_tweet.value_counts().plot(kind='bar', color=['#30a2da','#fc4f30','#e5ae38'])\n",
    "ax1.set_title(\"Types of tweets\",fontweight = 'bold', size=20)\n",
    "plt.xticks(rotation=0)\n",
    "plt.margins(0.05)\n",
    "\n",
    " \n",
    "#account info\n",
    "ax2 = plt.subplot2grid((8,2), (0,0)) \n",
    "style1 = dict(size=18, color='black')\n",
    "ax2.text(0, 0.8, str(\"Name: \"), **style1)\n",
    "ax2.text(0, 0.6, \"Creation: \", **style1)\n",
    "ax2.text(0, 0.4, \"Following: \", **style1)\n",
    "ax2.text(0, 0.2, \"Followers: \", **style1)\n",
    "\n",
    "style2 = dict(size=18, color='black',fontweight = 'bold')\n",
    "ax2.text(0.3, 0.8, str(name), **style2)\n",
    "ax2.text(0.3, 0.6, str(date_creation[4:10].strip()+date_creation[-5:]), **style2)\n",
    "ax2.text(0.3, 0.4, str(nfollowing), **style2)\n",
    "ax2.text(0.3, 0.2, str(nfollowers), **style2)\n",
    "\n",
    "ax2.axis('off')\n",
    "ax2.grid(False)\n",
    "ax2.set_title(\"Account information\",fontweight = 'bold', size=20)\n",
    "\n",
    "#type of tweets timeseries\n",
    "ax7 = plt.subplot2grid((8,2), (1,0), colspan=2)\n",
    "ax7.set_title(\"Type of tweeet - Time series last month\",fontweight = 'bold', size=20)\n",
    "ax7.plot_date(ts_reply.date, ts_reply['count'], label='replies',ls='-',marker='.')\n",
    "ax7.plot_date(ts_rt.date, ts_rt['count'], label='retweets',ls='-',marker='.')\n",
    "ax7.plot_date(ts.date, ts['count'], label='tweets',ls='-',marker='.')\n",
    "# set ticks to all days\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator())\n",
    "# limit axes\n",
    "import datetime\n",
    "start_date = ts.date.iloc[0] + datetime.timedelta(days=-1)\n",
    "end_date = ts.date.iloc[-1] + datetime.timedelta(days=+1)\n",
    "ax7.set_xlim([start_date,end_date])\n",
    "ax7.set_ylim(ymin=0)\n",
    "# format dates x labels\n",
    "xfmt = mdates.DateFormatter('%d-%m')\n",
    "ax7.xaxis.set_major_formatter(xfmt)\n",
    "\n",
    "\n",
    "# mark weekends\n",
    "def find_weekend_indices(datetime_array):\n",
    "    indices=[]\n",
    "    for i in range(len(datetime_array)):\n",
    "        if datetime_array[i].weekday()>=5:\n",
    "            indices.append(i)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "def highlight_weekend(weekend_indices,ax):\n",
    "    i=0\n",
    "    while i<len(weekend_indices):\n",
    "        ax7.axvspan(ts.date.iloc[weekend_indices[i]],\n",
    "                    ts.date.iloc[weekend_indices[i]+1],\n",
    "                    facecolor='green',\n",
    "                    edgecolor='none',\n",
    "                    alpha=.15)\n",
    "        i+=2\n",
    "        \n",
    "        \n",
    "    return None\n",
    "\n",
    "# replace month number with letters\n",
    "def correct_labels(ax):\n",
    "    labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "    days=[label.split(\" \")[0] for label in labels]\n",
    "    months=[\"Ja\",\"Fb\",\"Mr\",\"Ap\",\"My\",\"Jn\",\"Jl\",\"Ag\",\"Sp\",\"Oc\",\"Nv\",\"Dc\"]\n",
    "    final_labels=[]\n",
    "    for i in range(len(days)):\n",
    "        a=days[i].split(\"-\")\n",
    "        final_labels.append(a[0]+\"\\n\"+months[int(a[1])-1])\n",
    "    ax.set_xticklabels(final_labels)\n",
    "    \n",
    "fig.canvas.draw()\n",
    "correct_labels(ax7)\n",
    "highlight_weekend(find_weekend_indices(ts.date),ax7)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.margins(0.05)\n",
    "\n",
    "\n",
    "\n",
    "#worldcloud\n",
    "title = [\"All Tweets\", \"Regular Tweets\",\"Retweets\",\"Replies\"]\n",
    "place = [(2,0),(2,1),(3,0),(3,1)]\n",
    "i=0 # for plot titles\n",
    "for data in [words, words_regular, words_rt, words_reply]:  \n",
    "    #generate wordcloud\n",
    "    wordcloud = WordCloud(#stopwords=STOPWORDS,\n",
    "                          background_color='black',\n",
    "                          width=1800,\n",
    "                          height=1400\n",
    "                         ).generate(data)\n",
    "    #print wordcloud\n",
    "    plt.subplot2grid((8,2), place[i])\n",
    "    plt.imshow(wordcloud,extent=[0,100,0,1], aspect='auto')\n",
    "    #plt.extend(extent=[0,100,0,1], aspect='auto')\n",
    "    plt.title(title[i],fontweight = 'bold', size=20)\n",
    "    plt.axis('off')\n",
    "    plt.margins(0.05)\n",
    "    #plt.show()\n",
    "    i+=1\n",
    "\n",
    "ax3 = plt.subplot2grid((8,2), (4,0), rowspan=2) \n",
    "# hashtags barh plot\n",
    "bar_heights =  top20hash['count'].values\n",
    "bar_positions = np.arange(len(bar_heights))  \n",
    "tick_positions = range(0,20) \n",
    "\n",
    "ax3.barh(bar_positions, bar_heights, 0.5, align='center')#, color='red')\n",
    "ax3.set_yticks(tick_positions)\n",
    "ax3.set_yticklabels(top20hash['hashtag'].values,)\n",
    "ax3.set_yticks(tick_positions)\n",
    "#ax3.set_xlabel(\"Count\")\n",
    "#ax3.set_ylabel(\"Hashtags\")\n",
    "ax3.set_title(\"Top 20 Hashtags\",fontweight = 'bold', size=20)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.margins(0.05)\n",
    "\n",
    "ax4 = plt.subplot2grid((8,2), (4,1), rowspan=1)\n",
    "plt.pie(results.Prediction.value_counts(),\n",
    "        autopct='%1.1f%%', shadow=False,\n",
    "        startangle=90, labels=[\"Positive+Neutral\",\"Negative\"])#,colors=[\"blue\",\"red\"])\n",
    "plt.title(\"Sentiment Analysis\",fontweight = 'bold', size=20)\n",
    "plt.margins(0.05)\n",
    "\n",
    "ax5 = plt.subplot2grid((8,2), (5,1), rowspan=1)\n",
    "len_top4.plot(kind='bar',color='#6d904f')\n",
    "ax5.set_title(\"Languages\",fontweight = 'bold', size=20)\n",
    "plt.margins(0.05)\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "#map\n",
    "ax6 = plt.subplot2grid((8,2), (6,0), colspan=2, rowspan=2)\n",
    "m = Basemap()\n",
    "m.bluemarble() #m.etopo()\n",
    "m.drawcountries()\n",
    "\n",
    "# cities\n",
    "marker_size = [6,10,14,19,24]\n",
    "\n",
    "for loc in location_plot.index:\n",
    "    lat = loc.split()[0]\n",
    "    lon = loc.split()[1]\n",
    "    # bin 5 for clusters\n",
    "    count = int(location_plot.loc[loc]/10)\n",
    "    if count > 4: count=4\n",
    "    m.plot(lon,lat,'bo',markersize=marker_size[count], color='red',alpha=0.6,markeredgecolor='black',\n",
    "         markeredgewidth=1) \n",
    "\n",
    "labels = ['0-9', '10-19', '20-29', '30-39', '40+']\n",
    "\n",
    "leg = plt.legend(labels, ncol=1,fontsize=12, handlelength=2.5, loc=\"lower left\",\n",
    "                 borderpad = 1.8,handletextpad=1, title='Number of following:', scatterpoints = 1)\n",
    "leg.legendHandles[0]._legmarker.set_markersize(6)\n",
    "leg.legendHandles[1]._legmarker.set_markersize(9)\n",
    "leg.legendHandles[2]._legmarker.set_markersize(12)\n",
    "leg.legendHandles[3]._legmarker.set_markersize(15)\n",
    "leg.legendHandles[4]._legmarker.set_markersize(18)\n",
    "\n",
    "plt.title(\"Accounts you follow MAP\",fontweight = 'bold', size=20)\n",
    "plt.margins(0.05)\n",
    "\n",
    "plt.savefig('.\\\\images\\\\my_twitter.png', bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for sentiment analysis, what lenguate is it? use only english\n",
    "#LDA for topic modeling (code in scratch notebook: twitter.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
